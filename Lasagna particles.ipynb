{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "#These are mainly for opening objects \n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "#The real guts of the Deep learning\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne\n",
    "from lasagne import layers\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from lasagne.objectives import squared_error\n",
    "from nolearn.lasagne import NeuralNet\n",
    "from nolearn.lasagne import BatchIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Things you can change\n",
    "os.environ['LASAGNE_DATA_PATH'] = '/notebooks/ml-sims/data/nanoparticle/'\n",
    "numPart = 1000       #number of particles I will be tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now, let's load the particles\n",
    "\n",
    "totalPart = 262144   #total number of particles\n",
    "\n",
    "trainingFrac = 0.8\n",
    "validFrac = 0.1\n",
    "\n",
    "#Here, we are selecting random particles to spy on\n",
    "np.random.seed(0)\n",
    "indexes = np.random.choice(totPart, numPart)\n",
    "\n",
    "slice = np.s_[idxs, :]\n",
    "\n",
    "X = np.zeros((100, numPart*6))\n",
    "y = np.zeros((100, numPart*3))\n",
    "\n",
    "colors = ['b','g', 'r', 'y', 'k', 'm']\n",
    "\n",
    "absMinVel, absMaxVal = 0,0\n",
    "maxCoord = 10000\n",
    "\n",
    "for i in xrange(101):\n",
    "    fname = getFilename(i)\n",
    "    f = h5py.File(fname, 'r')\n",
    "    ids = f['PartType1']['ParticleIDs'][()]\n",
    "    sorter = ids.argsort() \n",
    "    \n",
    "    coords = f['PartType1']['Coordinates'][()] \n",
    "    coords = coords[sorter]#sort by ids\n",
    "    \n",
    "    #normalize coordinates (just divide by max)\n",
    "    coords/=maxCoord \n",
    "\n",
    "    #Here we test so see how the coordinates loaded\n",
    "    #from matplotlib import pyplot as plt\n",
    "    #plt.scatter(coords[0, 0], coords[0,1], c = colors[i%len(colors)]) \n",
    "\n",
    "    coords = coords[slice]\n",
    "    \n",
    "    if i!=0: \n",
    "        y[i-1,:] = coords.flatten() \n",
    "    if i!=100:\n",
    "        vels = f['PartType1']['Velocities'][()] \n",
    "        vels = vels[sorter]\n",
    "        \n",
    "        minVel, maxVel = vels.min(), vels.max()\n",
    "        if minVel < absMinVel:\n",
    "            absMinVel = minVel\n",
    "           \n",
    "        if maxVel > absMaxVal:\n",
    "            absMaxVal = maxVel\n",
    "\n",
    "        vels = vels[slice] \n",
    "        data = np.concatenate((coords, vels), axis = 1).flatten() \n",
    "                                                                                                \n",
    "        X[i,:] = data\n",
    "        del data     #Cleaning out the variables for next time      \n",
    "\n",
    "    del coords      #Still cleaning\n",
    "    f.close()       #Closing the file!\n",
    "\n",
    "    #Normalizing the Velocity coords\n",
    "    for n in xrange(nParticles):\n",
    "        for col in xrange(3):\n",
    "            X[:, n*6+3+col] = (X[:, n*6+3+col]-absMinVel)/(absMaxVal-absMinVel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading in the particle files\n",
    "def getFilename(i):          \n",
    "    base = path+'snapshot_' \n",
    "    if i<10:                 \n",
    "        out= base+'00%d.hdf5'%i\n",
    "        elif i<100:        \n",
    "        out= base+'0%d.hdf5'%i \n",
    "    else:           \n",
    "        out= base+'%d.hdf5'%i \n",
    "    return path+out       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def renormalize(array):\n",
    "    return (array - array.min()) / (array.max() - array.min())\n",
    "\n",
    "for i in range(5):\n",
    "    X[:, i, :, :] = renormalize(X[:, i, :, :])\n",
    "    \n",
    "y = renormalize(y)\n",
    "\n",
    "print(\"X.shape = {}, X.min = {}, X.max = {}\".format(X.shape, X.min(), X.max()))\n",
    "print(\"y.shape = {}, y.min = {}, y.max = {}\".format(y.shape, y.min(), y.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_PCA(array):\n",
    "\n",
    "    nimages0, nchannels0, height0, width0 = array.shape\n",
    "    rolled = np.transpose(array, (0, 2, 3, 1))\n",
    "    # transpose from N x channels x height x width  to  N x height x width x channels\n",
    "    nimages1, height1, width1, nchannels1 = rolled.shape\n",
    "    # check shapes\n",
    "    assert nimages0 == nimages1\n",
    "    assert nchannels0 == nchannels1\n",
    "    assert height0 == height1\n",
    "    assert width0 == width1\n",
    "    # flatten\n",
    "    reshaped = rolled.reshape(nimages1 * height1 * width1, nchannels1)\n",
    "    \n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    pca = PCA()\n",
    "    pca.fit(reshaped)\n",
    "    \n",
    "    cov = pca.get_covariance()\n",
    "    \n",
    "    eigenvalues, eigenvectors = np.linalg.eig(cov)\n",
    "    \n",
    "    return eigenvalues, eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AugmentedBatchIterator(BatchIterator):\n",
    "    \n",
    "    def __init__(self, batch_size, crop_size=8, testing=False):\n",
    "        super(AugmentedBatchIterator, self).__init__(batch_size)\n",
    "        self.crop_size = crop_size\n",
    "        self.testing = testing\n",
    "\n",
    "    def transform(self, Xb, yb):\n",
    "\n",
    "        Xb, yb = super(AugmentedBatchIterator, self).transform(Xb, yb)\n",
    "        batch_size, nchannels, width, height = Xb.shape\n",
    "        \n",
    "        if self.testing:\n",
    "            if self.crop_size % 2 == 0:\n",
    "                right = left = self.crop_size // 2\n",
    "            else:\n",
    "                right = self.crop_size // 2\n",
    "                left = self.crop_size // 2 + 1\n",
    "            X_new = Xb[:, :, right: -left, right: -left]\n",
    "            return X_new, yb\n",
    "\n",
    "        eigenvalues, eigenvectors = compute_PCA(Xb)\n",
    "\n",
    "        # Flip half of the images horizontally at random\n",
    "        indices = np.random.choice(batch_size, batch_size // 2, replace=False)        \n",
    "        Xb[indices] = Xb[indices, :, :, ::-1]\n",
    "\n",
    "        # Crop images\n",
    "        X_new = np.zeros(\n",
    "            (batch_size, nchannels, width - self.crop_size, height - self.crop_size),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # Choose x, y pixel posiitions at random\n",
    "            px, py = np.random.choice(self.crop_size, size=2)\n",
    "                \n",
    "            sx = slice(px, px + width - self.crop_size)\n",
    "            sy = slice(py, py + height - self.crop_size)\n",
    "            \n",
    "            # Rotate 0, 90, 180, or 270 degrees at random\n",
    "            nrotate = np.random.choice(4)\n",
    "            \n",
    "            # add random color perturbation\n",
    "            alpha = np.random.normal(loc=0.0, scale=0.5, size=5)\n",
    "            noise = np.dot(eigenvectors, np.transpose(alpha * eigenvalues))\n",
    "            \n",
    "            for j in range(nchannels):\n",
    "                X_new[i, j] = np.rot90(Xb[i, j, sx, sy] + noise[j], k=nrotate)\n",
    "                \n",
    "        return X_new, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SaveParams(object):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, nn, train_history):\n",
    "        if train_history[-1][\"valid_loss_best\"]:\n",
    "            nn.save_params_to(\"{}.params\".format(self.name))\n",
    "            with open(\"{}.history\".format(self.name), \"w\") as f:\n",
    "                pickle.dump(train_history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net49 = NeuralNet(\n",
    "    layers=[\n",
    "        ('input', layers.InputLayer),\n",
    "\n",
    "        ('conv11', layers.Conv2DLayer),\n",
    "        ('pool1', layers.MaxPool2DLayer),\n",
    "\n",
    "        ('conv21', layers.Conv2DLayer),\n",
    "        ('conv22', layers.Conv2DLayer),\n",
    "        ('pool2', layers.MaxPool2DLayer),\n",
    "\n",
    "        ('conv31', layers.Conv2DLayer),\n",
    "        ('conv32', layers.Conv2DLayer),\n",
    "        ('pool3', layers.MaxPool2DLayer),\n",
    "\n",
    "        ('hidden4', layers.DenseLayer),\n",
    "        ('dropout4', layers.DropoutLayer),\n",
    "        \n",
    "        ('hidden5', layers.DenseLayer),\n",
    "        ('dropout5', layers.DropoutLayer),\n",
    "\n",
    "        ('output', layers.DenseLayer),\n",
    "        ],\n",
    "    input_shape=(None, 5, 44, 44),\n",
    "    \n",
    "    conv11_num_filters=32, conv11_filter_size=(5, 5), \n",
    "    pool1_pool_size=(2, 2),\n",
    "\n",
    "    conv21_num_filters=64, conv21_filter_size=(3, 3),\n",
    "    conv22_num_filters=64, conv22_filter_size=(3, 3),\n",
    "    pool2_pool_size=(2, 2),\n",
    "\n",
    "    conv31_num_filters=128, conv31_filter_size=(3, 3),\n",
    "    conv32_num_filters=128, conv32_filter_size=(3, 3),\n",
    "    pool3_pool_size=(2, 2),\n",
    "\n",
    "    hidden4_num_units=2048,\n",
    "    dropout4_p=0.5,\n",
    "    \n",
    "    hidden5_num_units=2048,\n",
    "    dropout5_p=0.5,\n",
    "\n",
    "    output_num_units=1,\n",
    "    output_nonlinearity=None,\n",
    "\n",
    "    update_learning_rate=0.0001,\n",
    "    update_momentum=0.9,\n",
    "\n",
    "    objective_loss_function=squared_error,\n",
    "    regression=True,\n",
    "    max_epochs=1000,\n",
    "    batch_iterator_train=AugmentedBatchIterator(batch_size=128, crop_size=4),\n",
    "    batch_iterator_test=AugmentedBatchIterator(batch_size=128, crop_size=4, testing=True),\n",
    "\n",
    "    on_epoch_finished=[SaveParams(\"net49\")],\n",
    "\n",
    "    verbose=2,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
